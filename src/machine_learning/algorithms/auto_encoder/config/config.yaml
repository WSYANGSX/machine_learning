algorithm: "auto_encoder1"

training:
  batch_size: 256
  epochs: 200
  grad_clip: 5.0
  save_interval: 10

optimizer:
  type: "Adam"
  learning_rate: 1.0e-3
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-8
  weight_decay: 1.0e-5
  
scheduler:
  type: ReduceLROnPlateau
  factor: 0.5
  patience: 5  

data:
  data_path: "./src/machine_learning/data/minist"
  num_workers: 4
  norm_mean: 0.1307
  norm_std: 0.3081

model:
  initialize_weights: True

logging:
  log_interval: 10
  # log_dir: "./logs/auto_encoder/"
  model_dir: "./checkpoints/auto_encoder/"